\documentclass{article}

\usepackage[font=small]{caption}

\usepackage{nips_2016} 
\usepackage{xspace}
\usepackage{setspace}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx} % more modern
%\usepackage{subfigure} 
\usepackage{subcaption}
% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage[all]{xy}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{dsfont}
\usepackage{booktabs}
\usepackage{relsize}
\usepackage{siunitx}

\newcommand{\xsb}{\mathbf{\hat{x}}_{1,\ldots,B}}
\newcommand{\ysb}{\hat{y}_{1,\ldots,B}}
\newcommand{\xub}{\mathbf{x}_{1,\ldots,B}}


\newcommand{\xsj}{\mathbf{\hat{x}}_j}
\newcommand{\xsi}{\mathbf{\hat{x}}_i}
\newcommand{\ysi}{\hat{y}_i}
\newcommand{\ysj}{\hat{y}_j}

\newcommand{\xuj}{\mathbf{x}_j}
\newcommand{\xui}{\mathbf{x}_i}
\newcommand{\yui}{y_i}
\newcommand{\yuj}{y_j}
\newcommand{\sw}{s_{ \textsc{\relsize{-2}{\textsl{W}}} }}

\newcommand*{\eg}{e.g.\@\xspace}


% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2016}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:

%\title{Consistent Transduction for Unsupervised Domain Adaptation}
\title{Learning Transferrable Representations for Unsupervised Domain Adaptation}

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document} 

\maketitle

\begin{abstract} 
%<<<<<<< HEAD
%Supervised learning has enabled many interesting application due to the available large scale labelled datasets and very expressive models. In the absence of such a large dataset, domain adaptation is a promising direction to use a large labelled dataset as a reference and learn a model on unlabeled target dataset of interest. In this paper we approach the problem of unsupervised domain adaptation from a transduction perspective by considering the inference algorithm and target labels as part of the problem. We explicitly model the domain shift in terms of an asymmetric similarity metric and jointly solve for domain shift and target domain labels. We also show that our model can easily be extended for feature learning in order to learn features which are discriminative in the target domain. Our transductive model significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments.
%=======
Supervised learning with large scale labelled datasets and deep layered models has made a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers generalization issues under the presence of a domain shift between the training and the test data distribution. Since unsupervised domain adaptation algorithms directly address this domain shift problem between a labelled source dataset and an unlabelled target dataset, recent papers \cite{ganin15, tzeng14} have shown promising results by fine-tuning the networks with domain adaptation loss functions which try to align the mismatch between the training and testing data distributions.

Nevertheless, these recent deep learning based domain adaptation approaches still suffer from issues such as high sensitivity to the gradient reversal hyperparameters \cite{ganin15} and overfitting during the fine-tuning stage. In this paper, we propose a unified deep learning framework where the representation, cross domain transformation, and the target label inference is all jointly optimized in an end-to-end fashion for unsupervised domain adaptation. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin. We will make our learned models as well as the source code available immediately upon acceptance.
%>>>>>>> 495209cbd03d496cea6aa34df03ce5686deb5946
\end{abstract} 

\section{Introduction}
\label{intro}
Recently, deep convolutional neural networks \cite{alexnet, vggnet, googlenet} have propelled unprecedented advances in artificial intelligence including object recognition, speech recognition, and image captioning. Although these networks are very good at learning state of the art feature representations and recognize discriminative patterns, one of the major drawbacks is that the network requires huge labelled training data to fit the millions of parameters in the complex network. However, creating such datasets with complete annotations is not only tedious and error prone, but also extremely costly. In this regard, the research community has proposed different mechanisms such as semi-supervised learning \cite{semisup1,semisup2,semisup3}, transfer learning \cite{transfer1, transfer2}, weakly labelled learning, and domain adaptation. Among these approaches, domain adaptation is one of the most appealing techniques when a fully annotated dataset (e.g. ImageNet \cite{ImageNet}, Sports1M \cite{sports1m}) is already available as a reference. 

The goal of unsupervised domain adaptation, in particular, is: given a fully labeled source dataset and an unlabeled target dataset, to learn a model which can generalize to the target domain while taking the domain shift across the datasets into account. The majority of the literature \cite{gong12, baochen15, fernando13, baochen16, tommasi13} in unsupervised domain adaptation formulates a learning problem where the task is to find a transformation matrix to align the labelled source data distribution to the unlabelled target data distribution. Although these approaches have shown promising results, they show accuracy degradation because of the discrepancy between the learning procedure and the actual target inference procedure. In this paper, we aim to address this issue by incorporating the unknown target labels into the training procedure.

In this regard, we formulate a unified deep learning framework where the feature representation, domain transformation, and the target labels are all jointly optimized in an end-to-end fashion. The proposed framework first takes as input batch of labelled source and unlabelled target examples, maps the batch of raw input examples into deep representation. Then, the framework computes the loss of the input batch based on the two stage optimization where it alternates between inferring the labels of the target examples transductively and optimizing the domain transformation parameters.

Concretely, in the transduction stage, given the fixed domain transform parameter, we jointly infer all target labels by solving a discrete multi-label energy minimization problem. In the adaptation stage, given a fixed target label assignment, we seek to find the optimal asymmetric metric  between the source and the target data. The advantage of our method is that we can jointly learn the optimal feature representation and the optimal domain transformation parameter which are aware of the subsequent transductive inference procedure. 

Following the standard evaluation protocol in the unsupervised domain adaptation community, we evaluate our method on the digit classification task using MNIST \cite{mnist} and SVHN\cite{svhn} as well as the object recognition task using the Office \cite{office} dataset, and demonstrate state of the art performance in comparison to all existing unsupervised domain adaptation methods.  We will make our learned models as well as the source code available immediately upon acceptance.

\section{Related Work} 

This paper is closely related to two active research areas: (1) Unsupervised domain adaptation, and (2) Transductive learning.

\textbf{Unsupervised domain adaptation}: \cite{iccv15} casts the zero-shot learning \cite{zsl} problem as an unsupervised domain adaptation problem in dictionary learning and sparse coding framework assuming access to additional attribute information. Recently, \cite{activeNN} proposed the active nearest neighbor algorithm which combines the component of active learning into the domain adaptation problem and makes a bounded number of active queries to users. Also, \cite{gong12, fernando13, baochen16} proposed subspace alignment based approaches to unsupervised domain adaptation where the task is to learn a joint transformation and projection where the difference between the source and the target covariance is minimized. However, these methods learn the transform matrices on the whole source and target dataset without utilizing the source labels. 

\cite{tommasi13} utilizes local max margin metric learning objective \cite{lmnn} to first assign the target labels with the nearest neighbor scheme and then learn a distance metric to enforce that the negative pairwise distances are larger than the positive pairwise distances. However, this method learns a symmetric distance matrix shared by both the source and the target domains so the method is susceptible to the discrepancies between the source and the target distributions. Recently, \cite{ganin15, tzeng14} proposed a deep learning based method to learn domain invariant features by providing the reversed gradient signal from the binary domain classifiers. Although this method performs better than aforementioned approaches, their accuracy is limited since domain invariance does not necessarily imply discriminative features in the target domain. 

\textbf{Transductive learning}: In the transductive learning \cite{transduction}, the model has access to unlabelled test samples during training.  \cite{marcus} utilizes semi-supervised label propagation algorithm into the semi-supervised transfer learning problem assuming having access to few labeled examples and additional human specified semantic knowledge. \cite{coclassification} tackled a classification problem where predictions are made jointly across all test examples in a transductive \cite{transduction} setting. The method essentially enforces the notion that the true labels vary smoothly with respect to the input data. We extend this notion to infer the labels of unsupervised target data in a k-NN graph. 

To summarize, our main contribution is to formulate an end-to-end deep learning framework where we learn the optimal feature representation, infer target labels via discrete energy minimization (\textit{transduction}), and learn the  transformation (\textit{adaptation}) between source and target examples all jointly. Our experiments on digit classification using MNIST \cite{mnist} and SVHN\cite{svhn} as well as the object recognition experiments on Office \cite{office} datasets show state of the art results outperforming all existing methods by a substantial margin.


\section{Method} 
\input{method.tex}



\section{Experimental Results}
\input{exp.tex}
\section{Conclusion} 
%<<<<<<< HEAD
%We described a transductive approach to the unsupervised domain adaptation problem by defining a joint learning problem on the unsupervised target labels and an asymmetric similarity metric across the domains. We further described a method to learn features which are discriminative in the target domain. Experimental results on digit classification using MNIST\cite{mnist} and SVHN\cite{svhn} as well as on object recognition using Office\cite{office} dataset show state of the art performance with a significant margin.
%=======
We described an end-to-end deep learning framework for jointly optimizing the optimal deep feature representation, cross domain transformation, and the target label inference for state of the art unsupervised domain adaptation.

Experimental results on digit classification using MNIST\cite{mnist} and SVHN\cite{svhn} as well as on object recognition using Office\cite{office} dataset show state of the art performance with a significant margin. We will make our learned models as well as the source code available immediately upon acceptance.
%>>>>>>> 495209cbd03d496cea6aa34df03ce5686deb5946


\clearpage
\setlength{\bibsep}{2pt plus 0.25ex}
\begin{spacing}{0.95}
{\small
\bibliography{domain_transduction}
\bibliographystyle{abbrv}
}
\end{spacing}
\end{document} 

\section{Introduction}
\label{intro}
Recently, deep convolutional neural networks \cite{alexnet, vggnet, googlenet} have propelled unprecedented advances in artificial intelligence including object recognition, speech recognition, and image captioning. One of the major drawbacks of the method is that the network requires a lot of labelled training data to fit millions of parameters in the complex network model. However, creating such datasets with complete annotations is not only tedious and error prone, but also extremely costly. In this regard, the research community has proposed different mechanisms such as semi-supervised learning \cite{semisup1,semisup2,semisup3}, transfer learning \cite{transfer1, transfer2}, weakly labelled learning, and domain adaptation. Among these approaches, domain adaptation is one of the most appealing techniques when a fully annotated dataset (e.g. ImageNet \cite{ImageNet}, Sports1M \cite{sports1m}) is available as a reference. 

Formally, the goal of unsupervised domain adaptation is: given a fully labeled source dataset and an unlabeled target dataset, to learn a model which can generalize to the target domain while taking the domain shift across the datasets into account. The majority of the literature \cite{gong12, baochen15, fernando13, baochen16, tommasi13} in unsupervised domain adaptation formulates a learning problem where the task is to find a transformation matrix to align the labelled source data distribution to the unlabelled target data distribution. Although these approaches show promising results, they do not take the actual target inference procedure into the learning algorithm. We solve this problem by incorporating the unknown target labels into the training procedure.

Concretely, we formulate a unified framework where the domain transformation parameter and the target labels are jointly optimized in two alternating stages. In the transduction stage, given a fixed domain transform parameter, we jointly infer all target labels by solving a discrete multi-label energy minimization problem. In the adaptation stage, given a fixed target label assignment, we seek to find the optimal asymmetric metric  between the source and the target data. The advantage of our method is that we can learn a domain transformation parameter which is aware of the subsequent transductive inference procedure. 

Following the standard evaluation protocol in the domain adaptation community, we evaluate our method on the digit classification task using MNIST \cite{mnist} and SVHN\cite{svhn} as well as the object recognition task using the Office \cite{office} dataset, and demonstrate state of the art performance in comparison to all existing unsupervised domain adaptation methods.  We will make our learned models as well as the source code available immediately upon acceptance.

\section{Related Work} 

This chapter is closely related to two active research areas: (1) Unsupervised domain adaptation, and (2) Transductive learning.

\textbf{Unsupervised domain adaptation}: \cite{gong12, baochen15, fernando13, baochen16} proposed subspace alignment based approaches to unsupervised domain adaptation where the task is to learn a joint transformation and projection where the difference between the source and the target covariance is minimized. However, these methods learn the transform matrices on the whole source and target dataset without utilizing the source labels. 

\cite{tommasi13} utilizes local max margin metric learning objective \cite{lmnn} to first assign the target labels with the nearest neighbor scheme and then learn a distance metric to enforce that the negative pairwise distances are larger than the positive pairwise distances. However, this method learns a symmetric distance matrix shared by both the source and the target domains so the method is susceptible to the discrepancies between the source and the target distributions. Recently, \cite{ganin15, tzeng14} proposed a deep learning based method to learn domain invariant features by providing the reversed gradient signal from the binary domain classifiers. Although this method perform better than aforementioned approaches, their accuracy is limited since domain invariance does not necessarily imply discriminative features in the target domain. 

\textbf{Transductive learning}: In the transductive learning \cite{transduction}, the model has access to unlabelled test samples during training. Recently, \cite{coclassification} tackled a classification problem where predictions are made jointly across all test examples in a transductive \cite{transduction} setting. The method essentially enforces the notion that the true labels vary smoothly with respect to the input data. We extend this notion to infer the labels of unsupervised target data in a k-NN graph. 

To summarize, our main contribution is to formulate a joint optimization framework where we alternate between inferring target labels via discrete energy minimization (\textit{transduction}) and learning an asymmetric transformation (\textit{adaptation}) between source and target examples. Our experiments on digit classification using MNIST \cite{mnist} and SVHN\cite{svhn} as well as the object recognition experiments on Office \cite{office} datasets show state of the art results outperforming all existing methods by a substantial margin.


\section{Method} 
\input{./papers/da/method.tex}


\section{Experimental Results}
\input{./papers/da/exp.tex}
\section{Conclusion} 
%<<<<<<< HEAD
%We described a transductive approach to the unsupervised domain adaptation problem by defining a joint learning problem on the unsupervised target labels and an asymmetric similarity metric across the domains. We further described a method to learn features which are discriminative in the target domain. Experimental results on digit classification using MNIST\cite{mnist} and SVHN\cite{svhn} as well as on object recognition using Office\cite{office} dataset show state of the art performance with a significant margin.
%=======
We described a transductive approach to the unsupervised domain adaptation problem by defining a joint learning problem on the transductive target label assignment and an asymmetric similarity metric across the domains. We further described a method to learn deep features which are discriminative in the target domain. Experimental results on digit classification using MNIST\cite{mnist} and SVHN\cite{svhn} as well as on object recognition using Office\cite{office} dataset show state of the art performance with a significant margin. We will make our learned models as well as the source code available immediately upon acceptance.
%>>>>>>> 495209cbd03d496cea6aa34df03ce5686deb5946
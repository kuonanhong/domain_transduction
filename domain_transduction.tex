\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2016} 
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{dsfont}
\usepackage{booktabs}

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2016}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Robust Transduction for Unsupervised Adaptation}

\begin{document} 

\twocolumn[
\icmltitle{Robust Transduction for Unsupervised Adaptation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2016
% package.
\icmlauthor{Ozan Sener}{ozansener@cs.stanford.edu}
\icmladdress{Stanford University,
		     Stanford, CA 94304, USA}
		     \vspace{-2mm}
\icmladdress{Cornell University,
		     Ithaca, NY 14853, USA}
\icmlauthor{Hyun Oh Song}{hsong@cs.stanford.edu}
\icmladdress{Stanford University,
		     Stanford, CA 94304, USA}

\icmlauthor{Silvio Savarese}{ssilvio@stanford.edu}
\icmladdress{Stanford University,
		     Stanford, CA 94304, USA}
\icmlauthor{Ashutosh Saxena}{ashutosh@brainoft.com}
\icmladdress{Brain of Things,
		     Cupertino, CA 94304, USA}


% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{domain adaptation, transductive learning, metric learning, deep learning}

\vskip 0.3in
]

\begin{abstract} 
Abstract.
\end{abstract} 

\section{Introduction}
\label{intro}
Intro

\section{Related Work} 

This paper is closely related to two active research areas: (1) Unsupervised domain adaptation, and (2) Transductive learning.

\textbf{Unsupervised domain adaptation}: \cite{gong12, baochen15, fernando13, baochen16} proposed subspace alignment based approaches to unsupervised domain adaptation where the task is to learn a joint transformation and projection where the difference between the source and the target covariance is minimized. However, these method learn the transform matrices on the whole source and target dataset without utilizing the the source labels. 

\cite{tommasi13} utilizes local max margin metric learning objective \cite{lmnn} to first assign the target labels with nearest neighbor scheme and then learn a symmetric transform matrix to enforce that the negative pairwise distances are larger than the positive pairwise distances with a margin. However, this method learns a symmetric transform matrix shared by both the source and the target domains so the method is susceptible to the discrepancies between the source and the target distributions. Recently, \cite{ganin15, tzeng14} proposed a deep learning based method to learn domain invariant features via providing the reversed gradient signal from the binary domain classifiers.

\textbf{Transductive learning}: In transductive learning literature \cite{transduction}, the model has access to unlabelled test samples during training. Recently, \cite{coclassification} tackled a classification problem where predictions are made jointly across all test examples in a transductive \cite{transduction} setting. The method essentially enforces the notion that the true labels vary smoothly with respect to the input data. In our work, we extend this notion to infer the multiclass labels of unsupervised target data in a k-NN graph. 

To summarize, our main contribution is to formulate joint optimization framework where we alternate between inferring target labels via discrete energy minimization (\textit{transduction}) and learning asymmetric transformation between source and target examples. Our experiments on MNIST \cite{mint} and Office \cite{office} datasets show state of the art results outperforming all existing methods by a substantial margin.


\section{Method} 
\input{method.tex}


\section{Convergence Analysis}
It is monotonically decreasing after the each iteration

\section{Experimental Results}
\input{exp.tex}
\section{Conclusion} 


\bibliography{domain_transduction}
\bibliographystyle{icml2016}

\end{document} 
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{subfigure} 
\usepackage{subcaption}


% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2016} 
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{dsfont}
\usepackage{booktabs}
\usepackage{relsize}
\usepackage{lmodern}
\usepackage{slantsc}
\usepackage{siunitx}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{E}}}


\newcommand{\xsj}{\mathbf{\hat{x}}_j}
\newcommand{\xuj}{\mathbf{x}_j}
\newcommand{\xsi}{\mathbf{\hat{x}}_i}
\newcommand{\xui}{\mathbf{x}_i}
\newcommand{\ysi}{\hat{y}_i}
\newcommand{\ysj}{\hat{y}_j}
\newcommand{\yui}{y_i}
\newcommand{\sw}{s_{ \textsc{\relsize{-2}{\textsl{W}}} }}



% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2016}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Transductive Unsupervised Domain Adaptation}

\begin{document} 

\twocolumn[
\icmltitle{Transductive Unsupervised Domain Adaptation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2016
% package.
\icmlauthor{Ozan Sener}{ozansener@cs.stanford.edu}
\icmladdress{Stanford University,
		     Stanford, CA 94304, USA}
		     \vspace{-2mm}
\icmladdress{Cornell University,
		     Ithaca, NY 14853, USA}
\icmlauthor{Hyun Oh Song}{hsong@cs.stanford.edu}
\icmladdress{Stanford University,
		     Stanford, CA 94304, USA}

\icmlauthor{Silvio Savarese}{ssilvio@stanford.edu}
\icmladdress{Stanford University,
		     Stanford, CA 94304, USA}
\icmlauthor{Ashutosh Saxena}{ashutosh@brainoft.com}
\icmladdress{Brain of Things,
		     Cupertino, CA 94304, USA}


% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{domain adaptation, transductive learning, metric learning, deep learning}

\vskip 0.3in
]

\begin{abstract} 
Supervised learning has enabled many interesting application due to the available large scale labelled datasets and very expressive models. In the absence of such a large dataset, domain adaptation is a promising direction to use a large labelled dataset as a reference and learn a model on unlabeled target dataset of interest. In this paper we approach the problem of unsupervised domain adaptation from a transduction perspective by considering the inference algorithm and target labels as part of the problem. We explicitly model the domain shift in terms of an asymmetric similarity metric and jointly solve for domain shift and target domain labels. We also show that our model can easily be extended for feature learning in order to learn features which are discriminative in the target domain. Our transductive model significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments.
\end{abstract} 

\section{Introduction}
\label{intro}
Recently, deep convolutional neural networks \cite{alexnet, vggnet, googlenet} have propelled unprecedented advances in artificial intelligence including object recognition, speech recognition, and image captioning. One of the major drawbacks of the method is that the network requires a lot of labelled training data to fit millions of parameters in the complex network model. However, creating such datasets with complete annotations is not only tedious and error prone, but also extremely costly. In this regard, the research community has proposed different mechanisms such as semi-supervised learning \cite{semisup1,semisup2,semisup3}, transfer learning \cite{transfer1, transfer2}, weakly labelled learning, and domain adaptation. Among these approaches, domain adaptation is one of the most appealing techniques when a fully annotated dataset (e.g. ImageNet \cite{ImageNet}, Sports1M \cite{sports1m}) is available as a reference. 

Formally, the goal of unsupervised domain adaptation is: given a fully labeled source dataset and an unlabeled target dataset, to learn a model which can generalize to the target domain while taking the domain shift across the datasets into account. The majority of the literature \cite{gong12, baochen15, fernando13, baochen16, tommasi13} in unsupervised domain adaptation formulates a learning problem where the task is to find a transformation matrix to align the labelled source data distribution to the unlabelled target data distribution. Although these approaches show promising results, they do not take the actual target inference procedure into the learning algorithm. We solve this problem by incorporating the unknown target labels into the training procedure.

Concretely, we formulate a unified framework where the domain transformation parameter and the target labels are jointly optimized in two alternating stages. In the transduction stage, given a fixed domain transform parameter, we jointly infer all target labels by solving a discrete multi-label energy minimization problem. In the adaptation stage, given a fixed target label assignment, we seek to find the optimal asymmetric metric  between the source and the target data. The advantage of our method is that we can learn a domain transformation parameter which is aware of the subsequent transductive inference procedure. 

Following the standard evaluation protocol in the domain adaptation community, we evaluate our method on the digit classification task using MNIST \cite{mnist} and SVHN\cite{svhn} as well as the object recognition task using the Office \cite{office} dataset, and demonstrate state of the art performance in comparison to all existing unsupervised domain adaptation methods. 

We further share our learned models as well as the source code using TensorFlow\cite{tensorflow}. on \url{http://anonymized.webpage}

\section{Related Work} 

This paper is closely related to two active research areas: (1) Unsupervised domain adaptation, and (2) Transductive learning.

\textbf{Unsupervised domain adaptation}: \cite{gong12, baochen15, fernando13, baochen16} proposed subspace alignment based approaches to unsupervised domain adaptation where the task is to learn a joint transformation and projection where the difference between the source and the target covariance is minimized. However, these methods learn the transform matrices on the whole source and target dataset without utilizing the source labels. 

\cite{tommasi13} utilizes local max margin metric learning objective \cite{lmnn} to first assign the target labels with the nearest neighbor scheme and then learn a distance metric to enforce that the negative pairwise distances are larger than the positive pairwise distances with a margin. However, this method learns a symmetric distance matrix shared by both the source and the target domains so the method is susceptible to the discrepancies between the source and the target distributions. Recently, \cite{ganin15, tzeng14} proposed a deep learning based method to learn domain invariant features by providing the reversed gradient signal from the binary domain classifiers. Although this method perform better than aforementioned approaches, their accuracy is limited since domain invariance does not necessarily imply discriminative features in the target domain. 

\textbf{Transductive learning}: In the transductive learning literature \cite{transduction}, the model has access to unlabelled test samples during training. Recently, \cite{coclassification} tackled a classification problem where predictions are made jointly across all test examples in a transductive \cite{transduction} setting. The method essentially enforces the notion that the true labels vary smoothly with respect to the input data. In our work, we extend this notion to infer the multiclass labels of unsupervised target data in a k-NN graph. 

To summarize, our main contribution is to formulate a joint optimization framework where we alternate between inferring target labels via discrete energy minimization (\textit{transduction}) and learning an asymmetric transformation (\textit{adaptation}) between source and target examples. Our experiments on digit classification using MNIST \cite{mnist} and SVHN\cite{svhn} as well as the object recognition experiments on Office \cite{office} datasets show state of the art results outperforming all existing methods by a substantial margin.


\section{Method} 
\input{method.tex}


\section{Experimental Results}
\input{exp.tex}
\section{Conclusion} 
We described a transductive approach to the unsupervised domain adaptation problem by defining a joint learning problem on the unsupervised target labels and an asymmetric similarity metric across the domains. We further described a method to learn features which are discriminative in the target domain. Experimental results on digit classification using MNIST\cite{mnist} and SVHN\cite{svhn} as well as on object recognition using Office\cite{office} dataset show state of the art performance with a significant margin.



\clearpage
\bibliography{domain_transduction}
\bibliographystyle{icml2016}

\end{document} 
% !TEX root = domain_transduction.tex
%We address the problem of unsupervised transductive domain adaptation by jointly solving for the label assignment of unsupervised target domain as well as the shift domain shift in the following subsections. %We first define our model in Section~\ref{prob:def} and explain the two sub-problems of transduction and adaptation. We further explain the details of transduction in Section~\ref{label} and the details of adaptation in Section~\ref{metric}.
%Unsupervised domain adaptation is inherently a transduction problem since the main challenge lies in labeling the unsupervised data points with the help of the supervised data; because, adaptation is done offline without computationally concerns and the accuracy is critical since the transduction error is a good proxy for generalization \cite{adapttheory}. Main complication, which makes existing transduction methods inapplicable, is the domain shift between the supervised and unsupervised data points . We explicitly model the domain shift and jointly solve for unsupervised labels as well as the domain shift in a transductive learning setup.

\subsection{Problem Definition and Notation}
\label{prob:def}
In the unsupervised domain adaptation problem, one of the domains (source) is fully supervised $\{\xsi, \ysi \}_{i \in [N^s]}$ with $N^s$ data points $\xsi$ and corresponding labels $\ysi$ from a discrete set $\ysi \in \{1,\ldots, Y \}$.  The other domain (target), on the other hand is unsupervised and has $N^u$ data points $\{\xui \}_{i \in [N^u]}$. 

We further assume that both domains have different distributions $\xsi \sim p_s$ and $\xui \sim p_t$ defined on the same space as $\xsi,\xui \in \mathcal{X}$. We consider a case in which there are two feature functions  \mbox{$\Phi_s, \Phi_t:\mathcal{X}\rightarrow \mathcal{R}^d$} applicable to source and target separately. These feature functions extracts both the information shared among domains and explicit to the individual ones. The way we model common features is sharing a subset of parameters between feature functions as \mbox{$\Phi_s=\Phi_{\theta_c,\theta_s}$} and \mbox{$\Phi_t=\Phi_{\theta_c,\theta_t}$}. We use deep neural networks to implement these functions and in our implementation $\theta_c$ corresponds to the parameters in the first few layers of the networks and $\theta_s$, $\theta_t$ corresponds to the respective final layers. In general, our model to applicable any hierarchical and differentiable feature function which can be expressed as a composite function $\Phi_s = f_{\theta_s}(g_{\theta_c}(\cdot))$ for source and similarly for target.

\subsection{Consistent Structured Transduction}
% talk about cyclic consistency and structured consistency
Our method is based on jointly learning the transferable domain specific representations for source and target points as well as estimating the labels of the unsupervised data-points. We denote these two main components of our method as transduction and adaptation. The transduction is the sub-problem of labelling unsupervised data points and the adaptation is solving for the domain shift. In order to solve this joint problem tractably, we exploit two heuristics; cyclic consistency for transduction and structured consistency for adaptation. 

We use the 



\textbf{Cyclic consistency:} One desired property of  $\Phi_s$ and $\Phi_t$ is their consistency. If we estimate the labels of the unsupervised data points and further use these points to estimate the labels of supervised data-points, we want the predicted labels of the supervised data-points to be consistent with the ground truth labels. Using inner product as an asymmetric similarity metric -$s(\xsi, \xuj) = \Phi_s(\xsi)^\intercal \Phi(\xuj)$-,

\begin{displaymath}
    \xymatrix{
        (\xsi,\ysi) \ar@{<-}@<-2pt>`d[r] `[rrrrrr]|{\textbf{Cyclic Consistency: $\ysi = \ysi^{pred}$}} [rrrrrr]  \ar[rrr]|{\textbf{Transduction}}     &&& (\xui, \yui) \ar[rrr]|{\textbf{Transduction}} &&&  (\xsi,\ysi^{pred}) }
\end{displaymath}

It can be shown that; if the transduction from target to source follows a nearest neighbor rule, cyclic consistency can be enforced without explicitly computing $\ysi^{pred}$ using the large-margin nearest neighbor (LMNN)\cite{lmnn} rule. For each source point, we enforce a margin between its similarity with the nearest neighbor from the target having the same label and having a different label as; $ \Phi_s(\xsi)^\intercal \Phi_t(\mathbf{x}_{i^+}) > \Phi_s(\xsi)^\intercal \Phi_t(\mathbf{x}_{i^-}) + \alpha$ where $\mathbf{x}_{i^+}$ is the nearest target having the same class as $\xsi$ and $\mathbf{x}_{i^-}$ is the nearest target having a different class label. 


\textbf{Structured consistency:} We enforce a consistency in labelling of the target data-points during the transduction. In other words, if two target data-points are similar to each other, they are more likely to have same label. In order to enforce this consistency, we create a k-NN graph of target data points using a similarity metric $\Phi_t(\xui)^\intercal \Phi_t(\xuj)$. We denote the neighbors of the point $\xsi$ as $\mathcal{N}(\xsi)$. We enforce structured consistency by penalizing neighbor points of same labels with their similarity score. 

Our model leads to the following optimization problem, over the target labels $\yui$ and the feature function parameters $\theta_c, \theta_s, \theta_t$, jointly solving transduction and adaptation. 
\begin{equation}
\small
\begin{aligned}
\min_{\substack{\theta_c,\theta_s,\theta_t, \\ y_1, \ldots y_{N^u}}} &\underbrace{\sum_{i \in [N^s]} [\Phi_s(\xsi)^\intercal \Phi_t(\mathbf{x}_{i^-}) -\Phi_s(\xsi)^\intercal \Phi_t(\mathbf{x}_{i^+}) + \alpha]_{+}}_{\text{Cyclic Consistency}}  +\underbrace{\lambda \sum_{i \in [N^u]} \sum_{\xuj \in \mathcal{N}(\xui)}  \Phi_t(\xui)^\intercal \Phi_t(\xuj) \mathds{1}(y_i \neq y_j)}_{\text{Structured Consistency}}\\
% \Phi(\xui)^\intercal \Phi(\xuj) \mathds{1}(y_i \neq y_j)\\
s.t. &\quad i^{+} = {\arg\max}_{j | y_j = \hat{y}_i} \Phi_s(\xsi)^\intercal \Phi_t(\xuj) \quad \quad \text{and} \quad \quad  i^{-} = {\arg\max}_{j | y_j \neq \hat{y}_i}  \Phi_s(\xsi)^\intercal \Phi_t(\xuj)
\end{aligned}
\label{loss}
\end{equation}
where $\mathds{1}(a)$ is an indicator function which is $1$ if $a$ is true and $0$ otherwise. $[a]_+$ is a rectifier function which is equal to $\max(0, a)$.

We solve this optimization problem via alternating minimization through iterating over solving for unsupervised labels $y_i$(transduction) and learning the similarity metric $\theta_c,\theta_s,\theta_t$ (adaptation). We explain these two steps in detail in the following sections.


\iffalse
\begin{wrapfigure}{r}{0.5\textwidth}
    \begin{minipage}{0.5\textwidth}
    \vspace{-1cm}
\begin{algorithm}[H]
   \caption{Transduction with Domain Shift}
   \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries Input:} source $\mathbf{x}_i$, target $\mathbf{\hat{x}}_i$, $y_i$, batch size $B$
   \REPEAT
   \STATE  Sample $\{\mathbf{x^b}_{1 \cdots B}\}$, $\{\mathbf{\hat{x}}^b_{1 \cdots B}, \hat{y}^b_{1\cdots B}\}$
   \STATE Solve (\ref{robtran}) for $\{y_{1 \cdots B}\}$
   \FOR{$i=1$ {\bfseries to} $B$}
      \IF{$ \hat{y}_i \textbf{ in } y_{1 \cdots y_B} $} 
   \STATE Compute ($i^+, i^-$) using $\{y_{1 \cdots B}\}$ in (\ref{sup_nn})
   \STATE Update $\frac{\partial loss}{\partial \mathbf{\theta}}$ and  $\frac{\partial loss}{\partial \mathbf{W}} $ using (\ref{gradw},\ref{gradt})
   \ENDIF
   \ENDFOR
   \STATE $\mathbf{W} \leftarrow \mathbf{W} + \alpha \frac{\partial loss (y_i, \mathbf{W})}{\partial \mathbf{W}}$ 
   \STATE $\mathbf{\theta} \leftarrow \mathbf{\theta} + \alpha \frac{\partial loss (y_i, \mathbf{W})}{\partial \mathbf{\theta}}$
   \UNTIL CONVERGENCE or $MAX\_ITER$
\end{algorithmic}
\end{algorithm}
\vspace{-1cm}
  \end{minipage}
  \end{wrapfigure}
\fi





 %} $ 

%$\max_{y_1} \$
%$\min_{y_1} + 
%_4) $ assuming green is $0$, blue is $1$ and red is $2$

\subsection{Transduction: Labeling Target Domain}
\label{label}
In order to label the unsupervised points, we use the k-nearest-neighbor rule. We simply compute the k-NN supervised data point for each unsupervised data point using the learned metric and transfer the corresponding majority label. Formally, given a similarity metric $\theta_c, \theta_s, \theta_t$, the k-NN rule is 
$(y_i)^{pred} = \arg\max_y \frac{k_y(\xui)}{k}$ where $k_y(\xui)$ is the number of samples having label $y$ in the $k$ nearest neighbor of $\xui$ from the source set. 

One major issue with this approach is the accuracy of transduction during the initial stage of the algorithm. Since the learned metric will not be accurate, we expect to see some noisy k-NN sets. Hence, we propose two solutions to solve this problem.

\textbf{Structured Consistency:} We use label propagation to enforce consistency of the predicted labels of unsupervised data points. Similar to existing graph transduction algorithms \cite{label_prop1,label_prop2}, we create a k-nearest neighbor (k-NN) graph over the unsupervised data points and penalize disagreements of labels between neighbors.

\textbf{Reject option:} In the initial stage of the algorithm, we let transduction step to use reject $R$ as a label to unsupervised target points. In other words, our transduction algorithm can decide to not label some of the points so that they will not be used for adaptation. When the learned metric gets more accurate in the future iterations, transduction algorithm can label these rejected points.

We define our transduction sub-problem as:

\begin{equation}
\begin{aligned}
\min_{y_1, \ldots y_{N^u} \in [Y] \cap R}  &\sum_{i \in N^u} l(\xui, y_i) + \lambda \sum_{i \in N^u} \sum_{\xuj \in \mathcal{N}(\xui)} \Phi_s(\xui)^\intercal \Phi_t(\xuj)\mathds{1}(y_i \neq y_j)
\end{aligned}
\label{robtran}
\end{equation}

where $l(\xui, y) = \left\{ \begin{array}{cc}  1 - \frac{k_y(\xui)}{k} & y \in [Y] \\ \gamma \max_{y^\prime \in [Y]} \frac{k_y^\prime(\xui)}{k}
 & y=R \end{array}\right.$ and $\gamma$ is relative cost of the reject option. 
 
The $l(\xui,R)$ is smaller if none of the class has a majority, promoting reject option for undecided cases. We also modulate the $\gamma$ during learning to decrease number of reject options in the later stage of the adaptation. This problem can approximately be solved using many existing methods. We use the $\alpha$-$\beta$ swapping algorithm from \cite{kolmogrovalphabeta} since it is experimentally shown to be efficient and accurate. In order to further explain the label propagation, we visualize an example with $k=4$ and $4$-class classification problem in Figure~\ref{vis_label_prop}. 

\iffalse
\begin{wrapfigure}{r}{0.5\textwidth}
\includegraphics[width=0.5\textwidth]{fig11}
\caption{\textbf{Visualization of the Label Propogation.} We create a k-NN graph of unsupervised target points to enforce consistency with pairwise terms 
\mbox{$\psi_{ij}=sim(\mathbf{x}_i, \mathbf{x}_j) \mathds{1}(y_i \neq y_j)$} and use closest supervised source points to each class as 
\mbox{$ \psi_{i\hat{k}} \sw(\mathbf{\hat{x}}_k,\mathbf{x}_{i})$}.} 
\vspace{-1cm}
\label{vis_label_prop}
\end{wrapfigure}
\fi

It is also critical that this formulation requires solving high number of nearest neighbors which is computationally challenging. Fortunately, we use stochastic gradient descent in our adaptation stage with a carefully chosen batch size, which requires us to only solve the transduction over a batch. %Furthermore, we also efficiently implement the distance computation in the form of a few OpenBLAS calls as we further explain in detail in Section~\ref{imp_det}.

\subsection{Adaptation: Learning the Metric}
\label{metric}
Given the predicted labels $y_i$ for unsupervised data points $\xui$, we need to learn an asymmetric metric in order to minimize the loss function defined in (\ref{loss}). Following the cyclic consistency construction, the LMNN rule can be represented using the triplet loss defined with supervised data points and their closest same class and different class neighbors among the unsupervised points. We do not include the target-data points with reject label during this construction. Formally, we can define the adaptation problem give unsupervised labels as;
\begin{equation}
\begin{aligned}
\min_{\theta_c,\theta_s,\theta_t} &\sum_{i \in [N^s]} [\Phi_s(\xsi)^\intercal \Phi_t(\mathbf{x}_{i^-}) -\Phi_s(\xsi)^\intercal \Phi_t(\mathbf{x}_{i^+}) + \alpha]_{+} \\
\end{aligned}
\end{equation}
where $i^{+} = {\arg\max}_{j | y_j = \hat{y}_i} \Phi_s(\xsi)^\intercal \Phi_t(\xuj) \quad  \text{and} \quad   i^{-} = {\arg\max}_{j | y_j \neq \hat{y}_i, y_j \neq R}  \Phi_s(\xsi)^\intercal \Phi_t(\xuj)$

This minimization is convex if feature functions are convex. This is typically not the case with convolution neural networks; however, the gradient descent type algorithms are still applicable and works well in practice. Hence, we optimize this function via stochastic gradient descent using the sub-gradients $\frac{\partial loss}{\partial \theta_s}, \frac{\partial loss}{\partial \theta_t}$ and $\frac{\partial loss}{\partial \theta_c}$. These sub-gradients can be efficiently computed with back-propagation assuming $i^{+}$ and $i^{-}$ are given before the adaptation stage..
%\begin{equation}
%\frac{\partial loss}{\partial \theta_s} = \sum_{i \in [N^s]} \mathds{1}(\Phi_s(\xsi)^\intercal \Phi_t(\mathbf{x}_{i^+}) - \Phi_s(\xsi)^\intercal \Phi_t(\mathbf{x}_{i^-}) < \alpha) \left[ \frac{\partial \Phi_s(\xsi)^\intercal}{\theta_s} \left(\Phi_t(\mathbf{x}_{i^+}) -  \Phi_t(\mathbf{x}_{i^-})\right) \right]
%\end{equation}
%\begin{equation}
%\frac{\partial loss}{\partial \theta_t} = 
%\sum_{i \in [N^s]} \mathds{1}(\Phi_s(\xsi)^\intercal \Phi_t(\mathbf{x}_{i^+}) - \Phi_s(\xsi)^\intercal \Phi_t(\mathbf{x}_{i^-}) < \alpha) \left[ \left(\frac{\partial \Phi_t(\mathbf{x}_{i^+})^\intercal}{\theta_t} - \frac{\partial \Phi_t(\mathbf{x}_{i^-})^\intercal}{\theta_t} \right)  \Phi_s(\xsi) \right]
%\end{equation}
  
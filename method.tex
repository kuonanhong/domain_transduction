% !TEX root = domain_transduction.tex
Unsupervised domain adaptation is inherently a transduction problem since the main challenge lies in labeling the unsupervised data points with the help of the supervised data. Main complication is the domain shift between the supervised and unsupervised data points which makes existing transduction methods inapplicable. We explicitly model the domain shift and jointly solve for unsupervised labels as well as the domain shift in a fully transductive setup.

\subsection{Problem Definition}
In unsupervised domain adaptation problem, one of the domains is fully supervised $\{\xsi, \ysi \}_{i \in [N^s]}$ with $N^s$ data points $\xsi$ and corresponding labels $\ysi$ from a discrete set $\ysi \in \{1,\ldots, k \}$. Whereas, the other domain is unsupervised and only have $N^u$ data points $\{\xui \}_{i \in [N^u]}$. 


We further assume that both of the domains lie in the same space as $\xsi,\xui \in \mathcal{X}$ and there exist a feature function \mbox{$\Phi:\mathcal{X}\rightarrow \mathcal{R}^d$} which is applicable both of the domains. We separately study the case in which features function is a parametric function \mbox{$\Phi_\mathbf{\theta}:\mathcal{X}\rightarrow \mathcal{R}^d$} with a collection of parameters $\mathbf{\theta}$ to learn.

We explicitly model the domain shift in the form of a metric function and formulate our problem as a metric learning problem using an asymmetric similarity metric;
\begin{equation}
s(\xsi,\xuj) = \Phi(\xsi)^\intercal \mathbf{W} \Phi(\xuj)
\end{equation}
such that it is high if two data points $\xsj$ from the supervised and $\xui$ from the unsupervised domain are from the same class.

We model our learning setting in a fully transductive setting; in other words, the main purpose of the method is recovering the labels $\yui$ for each unsupervised example $\xui$. We consider the following objective function in order to compute $y_i$ as well as the similarity metric $\mathbf{W}$.

\begin{equation}
\begin{aligned}
\min_{\mathbf{W}, y_i} &\sum_{i \in [N^s]} &&[s(\xsi,\mathbf{x}_{i^-}) - s(\xsi,\mathbf{x}_{i^+}) + \alpha]_{+}  \\
&+\lambda &&\hspace{-3mm}\sum_{i \in N^u} \sum_{j \in \mathcal{N}(\xsi)}  \xui^\intercal \xuj \mathds{1}(y_i \neq y_j)\\
&s.t. \quad &&i^{+} = {\arg\max}_{j | y_j = \hat{y}_i} s(\mathbf{\hat{x}}_i,\mathbf{x}_{j}) \\
&\quad &&i^{-} = {\arg\max}_{j | y_j \neq \hat{y}_i} s(\mathbf{\hat{x}}_i,\mathbf{x}_{j}) 
\end{aligned}
\label{loss}
\end{equation}
where $\mathds{1}(a)$ is an indicator function which is $1$ if $a$ is true and $0$ otherwise and $[a]_+$ is a rectifier function which is equal to $\max(0, a)$. This construction is based on the triplet loss defined in \cite{lmnn} with transduction extension. Main rational behind the triplet loss is choosing a set of triplets in the form of an achor, positive and negative data points. It further enforces a margin $\alpha$ between the similarity of positive datapoint to anchor and the similarity of negative datapoint to the anchor. We also introduce a label consistency term which enforces that similar unsupervised data points should have same label. One important aspect of our construction is choosing the anchor data points strictly from supervised set and we discuss it in detail in Section~\ref{metric}.

We solve this optimization problem via alternating minimization through iterating over solving for unsupervised labels $y_i$ and the similarity metric $\mathbf{W}$. We explain these two steps the following sections.

\subsection{Labeling Unsupervised Points}
\label{label}
\begin{figure}[ht]
\includegraphics[width=\columnwidth]{figure1}
\caption{\textbf{Visualization of the Label Propogation.} Consider the unsupervised point $x_3$, the  resulting label would be:
\mbox{ ${\arg\min}_{y_3} -s(\hat{\mathbf{x}}_4,\mathbf{x}_3)\mathds{1}(\mathbf{y}_3=1) -s(\hat{\mathbf{x}}_{10},\mathbf{x}_3)\mathds{1}(y_3=2)$} $ -s(\hat{\mathbf{x}}_{11},\mathbf{x}_3)\mathds{1}(y_3=0) + \Phi(\mathbf{x}_2)^\intercal\phi(\mathbf{x}_3)\mathds{1}(y_3 \neq y_2) +\Phi(\mathbf{x}_3)^\intercal\phi(\mathbf{x}_4)\mathds{1}(y_3 \neq y_4) $ assuming green is $0$, blue is $1$ and red is $2$}
\label{vis_label_prop}
\end{figure}
In order to label the unsupervised data-points, we use nearest-neighbor(NN) rule as computing the NN supervised datapoint using the learned metric $s(\cdot,\cdot)$. Moreover, the NN rule will be accurate only if the metric is accurate enough. Since our algorithm is iterative alternating optimization, we still need rather accurate labeling even in the case of sub-optimal similarity metric. Hence, we introduce an additional robustness measure through label propogation. We first explain our NN-rule, then extend it to label propagation.

Given a similarity metric $s(\cdot,\cdot)$, the nearest neighbor rule is;
\begin{equation}
(y_i)^{pred} = \hat{y}_{{\arg\max}_j s(\mathbf{x_i}, \mathbf{\hat{x}_j})}
\end{equation}

The major issue for using the nearest neighbor rule is computationally efficiency. However, our implementation method and choice of parameters makes the NN rule tractable. We explain how we use stochastic gradient descent, select the batch size and efficiently implement the NN rule using OpenBLAS in Section\ref{imp_det}. 

In order to make our transduction stage robust, we use label propagation. Main idea behind our label propagation approach is enforcing consistency of the labels over unsupervised data points. In order to enforce this consistency, we create a k-NN graph over the unsupervised data points such that neighbor set $\mathcal{N}(\mathbf{x_i})$ for $\mathbf{x}_i$ is the k-unsupervised data point nearest to $\mathbf{x_i}$ using the $l_2$ distance in feature space. After the k-NN graph is created, we solve the following optimization problem for labeling unsupervised data points;

\begin{equation}
\begin{aligned}
{\arg\min}_{y_i}  &\sum_{i \in N^u} - \max_{\hat{y_j}=y_i}  s(\mathbf{\hat{x_j}},\mathbf{x_{i}}) \\
&+ \lambda
\sum_{i \in N^u} \sum_{j \in \mathcal{N}(\mathbf{x_i})} \mathbf{x_i}^T \mathbf{x_j}\mathds{1}(y_i \neq y_j)
\end{aligned}
\label{robtran}
\end{equation}
This problem is sub-modular and can easily be optimized through many methods like $\alpha$-$\beta$ swapping, quadratic pseudo-boolean optimization (QPBO), linear programming through roof-duality etc. We use $\alpha$-$\beta$ swapping algorithm from \cite{kolmogrovalphabeta}. In order to further explain the label propagation, we visualize a sample example with $k=2$ and $3$-class classification problem in Figure~\ref{vis_label_prop}. Since it is rather out-of-scope of this paper, we explain the details of the $\alpha$-$\beta$ swapping algorithm to the appendix.

\subsection{Learning Similarity Metric}
\label{metric}
Given the predicted labels $y_i$ for unsupervised data points $\mathbf{x_i}$, we need to learn the asymmetric metric in order to optimize the loss function defined in (\ref{loss}). We extend the LMNN(Large Margin Nearest Neighbour)\cite{lmnn} construction to the multi-domain case in order to define our metric learning objective.

Main intuition behind our formulation is searching for a metric which will label the supervised data points correctly using the unsupervised data points and their predicted labels. Since at this stage we already have a predicted label for each unsupervised data points, we can estimate a label for the supervised data points using these predicted labels. We also have ground truth labels for the supervised data points and we can look for a metric which will maximize the accuracy. In other words, the over all metric learning is combination of;

\begin{itemize}
\item predicting $\hat{y}^{pred}_j$ using $\mathbf{x_i}, \mathbf{\hat{x}_j},\hat{y}_j$
\item learning $s(\cdot,\cdot)$ by penalizing  $(\hat{y}_j)^{pred} \neq \hat{y}_j$ 
\end{itemize}

Fortunately, this can be jointly solved by minimizing the triplet loss as we define through nearest same-class and different-class examples of each supervised datapoint from unsupervised data points. In other words, we find the nearest positive and negative examples through;
\begin{equation}
\begin{aligned}
&i^{+} = {\arg\max}_{j | y_j = \hat{y}_i} s(\mathbf{\hat{x_i}},\mathbf{x_{j}}) \\
&i^{-} = {\arg\max}_{j | y_j \neq \hat{y}_i} s(\mathbf{\hat{x_i}},\mathbf{x_{j}}) 
\label{sup_nn}
\end{aligned}
\end{equation}
Then, we construct the triplet loss function with regularizer as;
\begin{equation}
\min_{\mathbf{W}, y_i} \sum_{i \in [N^s]} [s(\mathbf{\hat{x_i}},\mathbf{x_{i^-}}) - s(\mathbf{\hat{x_i}},\mathbf{x_{i^+}}) + \alpha]_{+} + r(\mathbf{W})
\end{equation}
which is convex in terms of the $\mathbf{W}$ if the regularizer is convex; and we optimize it by using Stochastic gradient descent through the subgradient $\frac{\partial loss (y_i, \mathbf{W})}{\partial \mathbf{W}} = \frac{\partial r ( \mathbf{W})}{\partial \mathbf{W}} + $
%\begin{small}
\begin{equation}
\sum_{i \in [N^s]} \mathds{1}(s(\mathbf{\hat{x_i}},\mathbf{x_{i^-}}) - s(\mathbf{\hat{x_i}},\mathbf{x_{i^+}})>\alpha) \left( \mathbf{\hat{x_i}}\mathbf{x_{i^-}}^\intercal - \mathbf{\hat{x_i}}\mathbf{x_{i^+}}^\intercal  \right)  
\label{gradw}
\end{equation}
%\end{small}
As a regularizer we are using the Frobenius norm of the similarity matrix as $r(\mathbf{W})=\frac{1}{2}\|\mathbf{W}\|_F^2$. We explain the details of this optimization routine and how we implement in the Section~\ref{implementation}.
\subsection{Learning Features}
In Section~\ref{label}~and~\ref{metric}, we developed our transductive labeling method with propogation and the way to learn the metric $s(\cdot,\cdot)$ through learning $\mathbf{W}$. During this formulation, we used a pre-defined feature function $\Phi$. However, the current trends in machine learning suggests that learning this feature function $\Phi$ from the data directly is a promising direction especially for visual data points. Hence, we consider the case $\Phi_{\mathbf{\Theta}}$ is a parametrized feature function with parameter set $\mathbf{\Theta}$. A typical example is CNNs(convolutional neural networks) with $\mathbf{\Theta}$ as concatenation of weights and biases. We update the feature weights as part of the metric learning. Hence, the gradient update for $\mathbf{\Theta}$ becomes; $\frac{\partial loss (y_i, \mathbf{W})}{\partial \mathbf{\Theta}} =$

\begin{small}
\begin{equation}
 \sum_{i \in [N^s]} \mathds{1}(s(\mathbf{\hat{x_i}},\mathbf{x_{i^-}}) - s(\mathbf{\hat{x_i}},\mathbf{x_{i^+}})) \left(\frac{\partial s(\mathbf{\hat{x_i}},\mathbf{x_{i^-}}) }{\partial \mathbf{\Theta}} - \frac{\partial s(\mathbf{\hat{x_i}},\mathbf{x_{i^+}}) }{\partial \mathbf{\Theta}} \right)
 \label{gradt}
\end{equation}
\end{small}


\begin{algorithm}[tb]
   \caption{Robust Transduction with Metric Learning}
   \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries Input:} unsupervised $\mathbf{x}_i$, supervised $\mathbf{\hat{x}}_i$, $y_i$, batch size $B$
   \REPEAT
   \STATE  Sample $(\mathbf{x}^b_1,\ldots \mathbf{x^b}_B)$, $(\mathbf{\hat{x}}^b_1,\ldots \mathbf{\hat{x}}^b_B)$, $(\hat{y}^b_1,\ldots \hat{x}^b_B)$
   \STATE Solve (\ref{robtran}) using $(\mathbf{x}^b_1,\ldots \mathbf{x^b}_B)$ and  $(\mathbf{\hat{x}}_1,\ldots \mathbf{\hat{x}}_{N^s})$
   \FOR{$i=1$ {\bfseries to} $B$}
      \IF{$ \hat{y}_i \textbf{ in } y_1 \ldots y_B $} 
   \STATE Compute ($i^+, i^-$) via $(\mathbf{x}^b_1,\ldots \mathbf{x^b}_B)$, $(\mathbf{\hat{x}}^b_1,\ldots \mathbf{\hat{x}}^b_B)$ 
   \STATE Update $\frac{\partial loss (y_i, \mathbf{W})}{\partial \mathbf{\Theta}}$ and  $\frac{\partial loss (y_i, \mathbf{W})}{\partial \mathbf{W}} $ using (\ref{gradw},\ref{gradt})
   \ENDIF
   \ENDFOR
   \STATE $\mathbf{W} \leftarrow \mathbf{W} + \alpha \frac{\partial loss (y_i, \mathbf{W})}{\partial \mathbf{W}}$ 
   \STATE $\mathbf{\Theta} \leftarrow \mathbf{\Theta} + \alpha \frac{\partial loss (y_i, \mathbf{W})}{\partial \mathbf{\Theta}}$
   \UNTIL CONVERGENCE or $MAX\_ITER$
\end{algorithmic}
\end{algorithm}

\subsection{Implementation Details}

\subsection{Weakly-Supervised Case}
